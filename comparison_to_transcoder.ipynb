{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying Computation in Superposition to transcoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.insert(0, os.path.abspath('../transcoder_circuits/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transcoder_circuits.circuit_analysis import *\n",
    "from transcoder_circuits.feature_dashboards import *\n",
    "from transcoder_circuits.replacement_ctx import *\n",
    "\n",
    "from sae_training.sparse_autoencoder import SparseAutoencoder\n",
    "from utils import tokenize_and_concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from einops import *\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = 'mps'\n",
    "else:\n",
    "    device = 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model & data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_lens import HookedTransformer, utils\n",
    "model = HookedTransformer.from_pretrained('gpt2-small').to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load transcoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcoder_template = \"../dufensky_transcoders/final_sparse_autoencoder_gpt2-small_blocks.{}.ln2.hook_normalized_24576\"\n",
    "transcoders = []\n",
    "sparsities = []\n",
    "for i in range(12):\n",
    "    transcoders.append(SparseAutoencoder.load_from_pretrained(f\"{transcoder_template.format(i)}.pt\").eval())\n",
    "    sparsities.append(torch.load(f\"{transcoder_template.format(i)}_log_feature_sparsity.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcoder_layer = 8\n",
    "transcoder_template = \"../dufensky_transcoders/final_sparse_autoencoder_gpt2-small_blocks.{}.ln2.hook_normalized_24576\"\n",
    "transcoder = SparseAutoencoder.load_from_pretrained(f\"{transcoder_template.format(transcoder_layer)}.pt\").to(device).eval()\n",
    "sparsity = torch.load(f\"{transcoder_template.format(transcoder_layer)}_log_feature_sparsity.pt\")\n",
    "live_features = np.arange(len(sparsity))[utils.to_numpy(sparsity > -4)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find transcoder's intermediate vectors\n",
    "For an encoder directions $\\mathbf{e}_i$ with activation $z_i(x)$, we can find directions $\\mathbf{d}_i$ such that $Wx \\approx \\sum_i z_i(x) \\mathbf{d}_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "torch.mps.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from midcoder import MidcoderConfig, Midcoder\n",
    "\n",
    "config = MidcoderConfig()\n",
    "config.device = device\n",
    "config.batch_size = 64\n",
    "config.steps_per_epoch = 10\n",
    "config.train_tokens = 100_000\n",
    "config.log = False\n",
    "midcoder = Midcoder({'model':model, 'transcoder':transcoder}, transcoder_layer, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "midcoder.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare decoder features to predicted values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from midcoder import MidcoderConfig, Midcoder\n",
    "\n",
    "config = MidcoderConfig()\n",
    "config.device = 'mps'\n",
    "config.mid_dim = 'd_mlp'\n",
    "midcoder = Midcoder({'model':model, 'transcoder':transcoder}, transcoder_layer, config)\n",
    "\n",
    "path = '../data/midcoder_layer8_v4_200M_tokens.pt'\n",
    "midcoder.load_weights(path, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_ids = midcoder.feat_ids.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparsity_mask = sparsity[feat_ids] > -4\n",
    "\n",
    "x = np.arange(len(sparsity))\n",
    "y = sparsity.sort().values\n",
    "plt.plot(x,y)\n",
    "plt.xlabel('rank')\n",
    "plt.ylabel('Log freq')\n",
    "plt.title('Feature sparsity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "act_gate_counts = midcoder.gate_act_counts[sparsity_mask]\n",
    "act_gate_mean = midcoder.act_gate_sum[sparsity_mask]/midcoder.act_counts[sparsity_mask].unsqueeze(1)\n",
    "act_gate_mean = act_gate_mean.detach()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(5,4), dpi=150)\n",
    "plt.hist(act_gate_mean[:500].flatten().cpu(), 100)\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Elementwise E[act * gate | act > 0]')\n",
    "plt.ylabel('Counts')\n",
    "plt.title('Distribution of activation*gate averages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mid_vector = midcoder.W_mid[feat_ids][sparsity_mask].detach()\n",
    "b_mid = midcoder.b_mid\n",
    "\n",
    "gate_mean = midcoder.gate_act_counts[sparsity_mask].detach() / midcoder.act_counts[sparsity_mask].unsqueeze(1)\n",
    "\n",
    "pre_relu_mean = midcoder.pre_relu_sum.detach() / (midcoder.counts)\n",
    "# mlp_out_mean = midcoder.mlp_out_sum.detach() / (midcoder.counts)\n",
    "# b_out = midcoder.b_dec_out.detach()\n",
    "# b_mid_out = midcoder.b_mid_out.detach()\n",
    "\n",
    "pred_decoder = (act_gate_mean * mid_vector) @ midcoder.W_out\n",
    "decoder = midcoder.W_dec[feat_ids][sparsity_mask].detach()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_sims = torch.nn.functional.cosine_similarity(decoder, pred_decoder, dim=1)\n",
    "\n",
    "freq = midcoder.act_counts[sparsity_mask] / midcoder.counts\n",
    "\n",
    "plt.figure(figsize=(8,3.5), dpi=150, layout='tight')\n",
    "plt.subplot(1,2,1)\n",
    "plt.hist(cos_sims.cpu(), 500)\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Cosine Similarity')\n",
    "plt.ylabel('Counts')\n",
    "plt.suptitle('Comparison of decoder and predicted decoder')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "# plt.plot(sparsity[sparsity_mask], cos_sims.cpu(), '.', alpha=0.03)\n",
    "plt.plot(freq.cpu(), cos_sims.cpu(), '.', alpha=0.1)\n",
    "plt.xlabel('Activation Frequency')\n",
    "plt.ylabel('Cosine Similarity')\n",
    "plt.xscale('log')\n",
    "# plt.title('Comparison of decoder and predicted decoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = sparsity[feat_ids][sparsity_mask] > -2\n",
    "\n",
    "plt.figure(figsize=(9,3.5), dpi=150, layout='tight')\n",
    "plt.subplot(1,2,1)\n",
    "sims = []\n",
    "\n",
    "idxs = range(10)\n",
    "for k in idxs:\n",
    "    dec = decoder[mask][k]\n",
    "    pred = pred_decoder[mask][k]\n",
    "    dec /= dec.norm()\n",
    "    pred /= pred.norm()\n",
    "    sims.append(f'{cos_sims[mask][k].item():.2f}')\n",
    "    plt.plot(dec.cpu(), pred.cpu(),'.', label=f'{cos_sims[mask][k].item():.2f}')\n",
    "vec = np.arange(-0.15, 0.15, 0.01)\n",
    "plt.plot(vec, vec, 'k--')\n",
    "plt.legend(title='Cos Sims', prop={'size':7})\n",
    "plt.ylabel('Predicted decoder')\n",
    "plt.xlabel('Decoder')\n",
    "plt.title('Elementwise comparison')\n",
    "plt.xlim((-0.15,0.15))\n",
    "plt.ylim((-0.15, 0.15))\n",
    "\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "for k in idxs:\n",
    "    log_counts = torch.log10(act_gate_counts[mask][k] + 1e-1)\n",
    "    plt.hist(log_counts.cpu(), 50, histtype='step')\n",
    "plt.yscale('log')\n",
    "plt.ylabel('')\n",
    "plt.xlabel('Log10( Counts for Act > 0 and Gate = 1)', fontsize=12)\n",
    "plt.title('Distribution of counts')\n",
    "\n",
    "plt.suptitle('High freq features')\n",
    "print(sims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (sparsity[feat_ids][sparsity_mask] > -3) & (sparsity[feat_ids][sparsity_mask] < -2.5)\n",
    "\n",
    "plt.figure(figsize=(9,3.5), dpi=150, layout='tight')\n",
    "plt.subplot(1,2,1)\n",
    "sims = []\n",
    "\n",
    "idxs = range(10)\n",
    "for k in idxs:\n",
    "    dec = decoder[mask][k]\n",
    "    pred = pred_decoder[mask][k]\n",
    "    dec /= dec.norm()\n",
    "    pred /= pred.norm()\n",
    "    sims.append(f'{cos_sims[mask][k].item():.2f}')\n",
    "    plt.plot(dec.cpu(), pred.cpu(),'.', label=f'{cos_sims[mask][k].item():.2f}')\n",
    "vec = np.arange(-0.15, 0.15, 0.01)\n",
    "plt.plot(vec, vec, 'k--')\n",
    "plt.legend(title='Cos Sims', prop={'size':7})\n",
    "plt.ylabel('Predicted decoder')\n",
    "plt.xlabel('Decoder')\n",
    "plt.title('Elementwise comparison')\n",
    "plt.xlim((-0.15,0.15))\n",
    "plt.ylim((-0.15, 0.15))\n",
    "\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "for k in idxs:\n",
    "    log_counts = torch.log10(act_gate_counts[mask][k] + 1e-1)\n",
    "    plt.hist(log_counts.cpu(), 50, histtype='step')\n",
    "plt.yscale('log')\n",
    "plt.ylabel('')\n",
    "plt.xlabel('Log10( Counts for Act > 0 and Gate = 1)', fontsize=12)\n",
    "plt.title('Distribution of counts')\n",
    "\n",
    "plt.suptitle('Med freq features')\n",
    "\n",
    "print(sims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = cos_sims < 0.7\n",
    "\n",
    "plt.figure(figsize=(8,3.5), dpi=150, layout='tight')\n",
    "plt.subplot(1,2,1)\n",
    "sims = []\n",
    "\n",
    "idxs = range(8)\n",
    "for k in idxs:\n",
    "    dec = decoder[mask][k]\n",
    "    pred = pred_decoder[mask][k]\n",
    "    dec /= dec.norm()\n",
    "    pred /= pred.norm()\n",
    "    sims.append(f'{cos_sims[mask][k].item():.2f}')\n",
    "    plt.plot(dec.cpu(), pred.cpu(),'.', label=f'{cos_sims[mask][k].item():.2f}')\n",
    "vec = np.arange(-0.15, 0.15, 0.01)\n",
    "plt.plot(vec, vec, 'k--')\n",
    "plt.legend(title='Cos Sims', prop={'size':7})\n",
    "\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "for k in idxs:\n",
    "    log_counts = torch.log10(act_gate_counts[mask][k] + 1e-1)\n",
    "    plt.hist(log_counts.cpu(), 50, histtype='step')\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.suptitle('Features with low cosine similarity')\n",
    "\n",
    "print(sims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sing_prob.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_corr_log[0][:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_corr.topk()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pair_prob / sing_prob\n",
    "x = x - torch.eye(*x.shape).to('mps')\n",
    "px.imshow(x.cpu(), color_continuous_scale = 'RdBu', color_continuous_midpoint = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.topk(dim=1, k=3).indices[250:270]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sing_prob = (midcoder.act_counts / midcoder.counts).unsqueeze(1)\n",
    "pair_prob = midcoder.act_cov_counts / midcoder.counts\n",
    "\n",
    "sort_idxs = sparsity[feat_ids][sparsity_mask].argsort(descending=True)\n",
    "sing_prob = sing_prob[sparsity_mask][sort_idxs]\n",
    "pair_prob = pair_prob[sparsity_mask,:][:,sparsity_mask][sort_idxs,:][:,sort_idxs]\n",
    "\n",
    "sing_prob_var = sing_prob * (1 - sing_prob)\n",
    "prob_corr = (pair_prob - sing_prob @ sing_prob.T) / torch.sqrt(sing_prob_var @ sing_prob.T)\n",
    "px.imshow(prob_corr.cpu(), color_continuous_scale = 'RdBu', color_continuous_midpoint = 0, range_color=[-0.3,0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_sims = torch.nn.functional.cosine_similarity(decoder, pred_decoder, dim=1)\n",
    "\n",
    "pretrain_counts = midcoder.act_counts[sparsity_mask]\n",
    "\n",
    "plt.figure(figsize=(8,3.5), dpi=150, layout='tight')\n",
    "plt.subplot(1,2,1)\n",
    "plt.hist(cos_sims.cpu(), 500)\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Cosine Similarity')\n",
    "plt.ylabel('Counts')\n",
    "plt.suptitle('Comparison of decoder and predicted decoder')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "# plt.plot(sparsity[sparsity_mask], cos_sims.cpu(), '.', alpha=0.03)\n",
    "plt.plot(torch.arange(len(cos_sims)), cos_sims[sort_idxs].cpu(), '.', alpha=0.1)\n",
    "plt.xlim((-1,400))\n",
    "plt.xlabel('Pretrain Counts')\n",
    "plt.ylabel('Cosine Similarity')\n",
    "# plt.xscale('log')\n",
    "# plt.title('Comparison of decoder and predicted decoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(torch.arange(len(sing_prob)), sing_prob[:,0].cpu(), '.')\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask1 = sparsity[feat_ids][sparsity_mask] > -2\n",
    "mask2 = (sparsity[feat_ids][sparsity_mask] > -3.5) & (sparsity[feat_ids][sparsity_mask] < -3)\n",
    "num = 100\n",
    "\n",
    "dec1 = decoder[mask1]\n",
    "dec1 /= dec1.norm(dim=1, keepdim=True)\n",
    "dec2 = decoder[mask2][:num]\n",
    "dec2 /= dec2.norm(dim=1, keepdim=True)\n",
    "\n",
    "sims11 = einsum(dec1, dec1, \"f1 d, f2 d -> f1 f2\")\n",
    "sims12 = einsum(dec1, dec2, \"f1 d, f2 d -> f1 f2\")\n",
    "sims22 = einsum(dec2, dec2, \"f1 d, f2 d -> f1 f2\")\n",
    "\n",
    "sims11_nodiag = sims11 - torch.eye(*sims11.shape).to('mps')\n",
    "sims22_nodiag = sims22 - torch.eye(*sims22.shape).to('mps')\n",
    "print(sims11_nodiag.max(dim=0))\n",
    "print(sims12.max(dim=0))\n",
    "print(sims22_nodiag.max(dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask1 = sparsity[feat_ids][sparsity_mask] > -2\n",
    "mask2 = (sparsity[feat_ids][sparsity_mask] > -3.5) & (sparsity[feat_ids][sparsity_mask] < -3)\n",
    "num = 100\n",
    "\n",
    "dec1 = pred_decoder[mask1]\n",
    "dec1 /= dec1.norm(dim=1, keepdim=True)\n",
    "dec2 = pred_decoder[mask2][:num]\n",
    "dec2 /= dec2.norm(dim=1, keepdim=True)\n",
    "\n",
    "sims11 = einsum(dec1, dec1, \"f1 d, f2 d -> f1 f2\")\n",
    "sims12 = einsum(dec1, dec2, \"f1 d, f2 d -> f1 f2\")\n",
    "sims22 = einsum(dec2, dec2, \"f1 d, f2 d -> f1 f2\")\n",
    "\n",
    "sims11_nodiag = sims11 - torch.eye(*sims11.shape).to('mps')\n",
    "sims22_nodiag = sims22 - torch.eye(*sims22.shape).to('mps')\n",
    "print(sims11_nodiag.max(dim=0))\n",
    "print(sims12.max(dim=0))\n",
    "print(sims22_nodiag.max(dim=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder directions from gaussian noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_ids = midcoder.feat_ids.cpu()\n",
    "sparsity_mask = sparsity[feat_ids] > -3.5\n",
    "act_freqs = midcoder.act_counts.detach() / midcoder.counts.detach()\n",
    "act_freqs, sort_idxs = act_freqs.sort(descending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "act_mean = midcoder.act_sum[sort_idxs].detach() / midcoder.act_counts[sort_idxs].detach()\n",
    "act_sq_mean = midcoder.act_sq_sum[sort_idxs].detach() / midcoder.act_counts[sort_idxs].detach()\n",
    "act_var = act_sq_mean - act_mean**2\n",
    "\n",
    "mid_vec = midcoder.W_mid[feat_ids][sort_idxs].detach()\n",
    "b_mid = midcoder.b_mid.detach()\n",
    "\n",
    "mid_mean = midcoder.pre_relu_sum.detach() / midcoder.counts.detach()\n",
    "mid_cov = midcoder.pre_relu_cov_sum.detach() / midcoder.counts.detach()\n",
    "mid_cov = mid_cov - mid_mean.unsqueeze(1) @ mid_mean.unsqueeze(0)\n",
    "mid_var = mid_cov.diag()\n",
    "mid_corr = mid_cov / torch.sqrt(mid_var.unsqueeze(1) @ mid_var.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assuming covariances are negligible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mid_vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_mean = b_mid.unsqueeze(0) + mid_mean.unsqueeze(0) - act_mean.unsqueeze(1) * mid_vec\n",
    "noise_var = mid_var.unsqueeze(0) - act_var.unsqueeze(1) * mid_vec**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(act_mean.cpu(), act_var.cpu(),'.', alpha=0.3)\n",
    "vec = torch.arange(0.1, 15, 0.01)\n",
    "plt.plot(vec, vec**2,'--', label='Exponential Dist.') #expectation for exponential\n",
    "plt.legend()\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_dist = torch.distributions.exponential.Exponential(0.1)\n",
    "gauss_dist = torch.distributions.normal.Normal(0, 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check covariance structure of pre-relu values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.imshow(mid_cov[:100,:100].cpu(), color_continuous_scale = 'RdBu', color_continuous_midpoint = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_cov = midcoder.W_in.T  @ midcoder.W_in\n",
    "px.imshow(weight_cov[:100,:100].cpu(), color_continuous_scale = 'RdBu', color_continuous_midpoint = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numerology for sparsities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = sparsity.sort(descending=True).values\n",
    "s = 10**(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s[None,:200,None].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = s[:200,None] * s[None,:200]\n",
    "pairs = pairs.flatten()\n",
    "trips = s[:200,None,None] * s[None,:200,None] * s[None,None,:200]\n",
    "trips = trips.flatten()\n",
    "\n",
    "t = torch.cat((s[:200], pairs, trips)).sort(descending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(s)\n",
    "plt.plot(t[:25000])\n",
    "plt.yscale('log')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prod = s[:200].unsqueeze(0) * s[:200].unsqueeze(1)\n",
    "prod.flatten().sort().values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_ids = sparsity.argsort(descending=True)\n",
    "decoders = midcoder.W_dec[sort_ids]\n",
    "decoders /= decoders.norm(dim=1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id = 2000\n",
    "cos_sims = decoders[id] @ decoders.T\n",
    "\n",
    "plt.hist(cos_sims.cpu(), 100);\n",
    "plt.title(f'Id = {id}, log10(freq) = {sparsity[sort_idxs[id]]:.2f}')\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topk = cos_sims.abs().topk(k=30)\n",
    "topk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_sims2 = decoders[topk.indices] @ decoders[topk.indices].T\n",
    "px.imshow(cos_sims2.cpu(), color_continuous_scale = 'RdBu', range_color=[-1,1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_ids = midcoder.feat_ids.cpu()\n",
    "feat_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_sort_ids = []\n",
    "for id in feat_ids:\n",
    "    feat_sort_ids.append((sort_ids==id).nonzero().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_sort_ids = torch.tensor(feat_sort_ids)\n",
    "feat_sort_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bidict import bidict\n",
    "\n",
    "feat2sort_ids = bidict({})\n",
    "for feat_id in feat_ids:\n",
    "    feat_id = feat_id.item()\n",
    "    sort_id = (sort_ids==feat_id).nonzero().item()\n",
    "    feat2sort_ids[feat_id] = sort_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_sort_ids = sorted(list(feat2sort_ids.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Factorizing Decoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_ids = sparsity.argsort(descending=True)\n",
    "decoders = transcoder.W_dec[sort_ids].detach()\n",
    "decoders /= decoders.norm(dim=1, keepdim=True)\n",
    "decoders = decoders[:20000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(sparsity[sort_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from factorizer import Factorizer, FactorizerConfig\n",
    "\n",
    "cfg = FactorizerConfig()\n",
    "cfg.theta = 0.3\n",
    "cfg.factor_param = 0.5\n",
    "cfg.factors = 10000\n",
    "\n",
    "cfg.batch_size = 1000\n",
    "cfg.epochs = 100\n",
    "cfg.log = False\n",
    "cfg.device = 'mps'\n",
    "\n",
    "factorizer = Factorizer(cfg, decoders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factorizer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = 0.5\n",
    "top_feat = 10000\n",
    "\n",
    "n_feat = decoders.shape[0]\n",
    "C = (decoders[:top_feat] @ decoders[:top_feat].T).flatten()\n",
    "C[C<theta] = 0\n",
    "C = C.reshape(top_feat, top_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_cpu = C.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eig = np.linalg.eigh(C_cpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(eig.eigenvalues, '.-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(svd.S.cpu(),'.-')\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_cutoff = 1e-2\n",
    "S = svd.S\n",
    "mask = S >= s_cutoff\n",
    "Sinv = 1/S[mask].sqrt()\n",
    "Ainv = torch.diag(Sinv) @ svd.U[mask].T \n",
    "features = Ainv @ decoders[:top_feat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = svd.S > 1\n",
    "svd.U[:,mask] / svd.V[:,mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd.U.sum(dim=1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
