{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring the context dependence of transcoder features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.insert(0, os.path.abspath('../transcoder_circuits/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transcoder_circuits.circuit_analysis import *\n",
    "from transcoder_circuits.feature_dashboards import *\n",
    "from transcoder_circuits.replacement_ctx import *\n",
    "\n",
    "from sae_training.sparse_autoencoder import SparseAutoencoder\n",
    "from utils import tokenize_and_concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from einops import *\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = 'mps'\n",
    "else:\n",
    "    device = 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model & data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    }
   ],
   "source": [
    "from transformer_lens import HookedTransformer, utils\n",
    "model = HookedTransformer.from_pretrained('gpt2-small').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/datasets/load.py:1429: FutureWarning: The repository for Skylion007/openwebtext contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/Skylion007/openwebtext\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from huggingface_hub import HfApi\n",
    "\n",
    "dataset = load_dataset('Skylion007/openwebtext', split='train', streaming=True)\n",
    "dataset = dataset.shuffle(seed=42, buffer_size=10_000)\n",
    "# tokenized_owt = tokenize_and_concatenate(dataset, model.tokenizer, max_length=128, streaming=True)\n",
    "# tokenized_owt = tokenized_owt.shuffle(42)\n",
    "# tokenized_owt = tokenized_owt.take(12800*2)\n",
    "# owt_tokens = np.stack([x['tokens'] for x in tokenized_owt])\n",
    "# owt_tokens_torch = torch.tensor(owt_tokens).to('mps')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load transcoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcoder_template = \"../dufensky_transcoders/final_sparse_autoencoder_gpt2-small_blocks.{}.ln2.hook_normalized_24576\"\n",
    "transcoders = []\n",
    "sparsities = []\n",
    "for i in range(12):\n",
    "    transcoders.append(SparseAutoencoder.load_from_pretrained(f\"{transcoder_template.format(i)}.pt\").eval())\n",
    "    sparsities.append(torch.load(f\"{transcoder_template.format(i)}_log_feature_sparsity.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcoder_layer = 8\n",
    "transcoder_template = \"../dufensky_transcoders/final_sparse_autoencoder_gpt2-small_blocks.{}.ln2.hook_normalized_24576\"\n",
    "transcoder = SparseAutoencoder.load_from_pretrained(f\"{transcoder_template.format(transcoder_layer)}.pt\").to(device).eval()\n",
    "sparsity = torch.load(f\"{transcoder_template.format(transcoder_layer)}_log_feature_sparsity.pt\")\n",
    "live_features = np.arange(len(sparsity))[utils.to_numpy(sparsity > -4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find transcoder's intermediate vectors\n",
    "For an encoder directions $\\mathbf{e}_i$ with activation $z_i(x)$, we can find directions $\\mathbf{d}_i$ such that $Wx \\approx \\sum_i z_i(x) \\mathbf{d}_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [04:20<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "torch.mps.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from midcoder import MidcoderConfig, Midcoder\n",
    "\n",
    "config = MidcoderConfig()\n",
    "config.device = device\n",
    "config.batch_size = 64\n",
    "config.steps_per_epoch = 1\n",
    "config.epochs = 100\n",
    "config.log = True\n",
    "midcoder = Midcoder(model, transcoder, transcoder_layer, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "Python(70071) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Python(70072) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmichael_pearce\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "Python(70075) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Python(70076) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/michael/Library/CloudStorage/GoogleDrive-michaelttpearce@gmail.com/My Drive/AI Safety/Transcoders/transcoder_context/wandb/run-20240626_150919-dhxonod5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/michael_pearce/midcoder/runs/dhxonod5' target=\"_blank\">leafy-wave-1</a></strong> to <a href='https://wandb.ai/michael_pearce/midcoder' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/michael_pearce/midcoder' target=\"_blank\">https://wandb.ai/michael_pearce/midcoder</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/michael_pearce/midcoder/runs/dhxonod5' target=\"_blank\">https://wandb.ai/michael_pearce/midcoder/runs/dhxonod5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 2.504, loss_norm: 1.928:  44%|████▍     | 44/100 [01:49<02:18,  2.48s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmidcoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/CloudStorage/GoogleDrive-michaelttpearce@gmail.com/My Drive/AI Safety/Transcoders/transcoder_context/midcoder.py:128\u001b[0m, in \u001b[0;36mMidcoder.fit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    126\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(dataloader)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtokens\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    127\u001b[0m loss, loss_norm \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain()\u001b[38;5;241m.\u001b[39mstep(batch)\n\u001b[0;32m--> 128\u001b[0m epoch \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [(\u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, loss_norm\u001b[38;5;241m.\u001b[39mitem())]\n\u001b[1;32m    130\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    131\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Network error (ConnectionError), entering retry loop.\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n"
     ]
    }
   ],
   "source": [
    "midcoder.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = midcoder.get_dataloader(batch_size=4)\n",
    "batch = next(dataloader)['tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (73252 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "# inputs, outputs = mid_transcoder.get_inputs_outputs(batch)\n",
    "# mid, _ = mid_transcoder.forward(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'blocks.8.ln2.hook_normalized'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "midcoder.hook_point.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3072, 768])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.blocks[8].mlp.W_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, cache = midcoder.model.run_with_cache(batch, stop_at_layer=transcoder_layer+1, \n",
    "                                             names_filter=[midcoder.hook_point.name,\n",
    "                                                        'blocks.8.hook_mlp_out'])\n",
    "inputs = cache[midcoder.hook_point.name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 128, 768])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = cache['blocks.8.hook_mlp_out']\n",
    "outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 128, 768])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 128, 3072])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = inputs @ midcoder.W_in + midcoder.b_in\n",
    "outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_array = owt_tokens_torch[:128*1]\n",
    "batch_size = 128\n",
    "layer = 8\n",
    "_, cache = model.run_with_cache(token_array[i:i+batch_size], stop_at_layer=layer+1, names_filter=[\n",
    "\t\t\t\t\n",
    "\t\t\t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256, 128])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "owt_tokens_torch[:128*2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HookedTransformer(\n",
       "  (embed): Embed()\n",
       "  (hook_embed): HookPoint()\n",
       "  (pos_embed): PosEmbed()\n",
       "  (hook_pos_embed): HookPoint()\n",
       "  (blocks): ModuleList(\n",
       "    (0-11): 12 x TransformerBlock(\n",
       "      (ln1): LayerNormPre(\n",
       "        (hook_scale): HookPoint()\n",
       "        (hook_normalized): HookPoint()\n",
       "      )\n",
       "      (ln2): LayerNormPre(\n",
       "        (hook_scale): HookPoint()\n",
       "        (hook_normalized): HookPoint()\n",
       "      )\n",
       "      (attn): Attention(\n",
       "        (hook_k): HookPoint()\n",
       "        (hook_q): HookPoint()\n",
       "        (hook_v): HookPoint()\n",
       "        (hook_z): HookPoint()\n",
       "        (hook_attn_scores): HookPoint()\n",
       "        (hook_pattern): HookPoint()\n",
       "        (hook_result): HookPoint()\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (hook_pre): HookPoint()\n",
       "        (hook_post): HookPoint()\n",
       "      )\n",
       "      (hook_attn_in): HookPoint()\n",
       "      (hook_q_input): HookPoint()\n",
       "      (hook_k_input): HookPoint()\n",
       "      (hook_v_input): HookPoint()\n",
       "      (hook_mlp_in): HookPoint()\n",
       "      (hook_attn_out): HookPoint()\n",
       "      (hook_mlp_out): HookPoint()\n",
       "      (hook_resid_pre): HookPoint()\n",
       "      (hook_resid_mid): HookPoint()\n",
       "      (hook_resid_post): HookPoint()\n",
       "    )\n",
       "  )\n",
       "  (ln_final): LayerNormPre(\n",
       "    (hook_scale): HookPoint()\n",
       "    (hook_normalized): HookPoint()\n",
       "  )\n",
       "  (unembed): Unembed()\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'blocks.8.mlp.hook_pre'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.blocks[layer].mlp.hook_pre.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['OV',\n",
       " 'QK',\n",
       " 'T_destination',\n",
       " 'W_E',\n",
       " 'W_E_pos',\n",
       " 'W_K',\n",
       " 'W_O',\n",
       " 'W_Q',\n",
       " 'W_U',\n",
       " 'W_V',\n",
       " 'W_gate',\n",
       " 'W_in',\n",
       " 'W_out',\n",
       " 'W_pos',\n",
       " '__annotations__',\n",
       " '__call__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_apply',\n",
       " '_backward_hooks',\n",
       " '_backward_pre_hooks',\n",
       " '_buffers',\n",
       " '_call_impl',\n",
       " '_compiled_call_impl',\n",
       " '_forward_hooks',\n",
       " '_forward_hooks_always_called',\n",
       " '_forward_hooks_with_kwargs',\n",
       " '_forward_pre_hooks',\n",
       " '_forward_pre_hooks_with_kwargs',\n",
       " '_get_backward_hooks',\n",
       " '_get_backward_pre_hooks',\n",
       " '_get_name',\n",
       " '_is_full_backward_hook',\n",
       " '_load_from_state_dict',\n",
       " '_load_state_dict_post_hooks',\n",
       " '_load_state_dict_pre_hooks',\n",
       " '_maybe_warn_non_full_backward_hook',\n",
       " '_modules',\n",
       " '_named_members',\n",
       " '_non_persistent_buffers_set',\n",
       " '_parameters',\n",
       " '_register_load_state_dict_pre_hook',\n",
       " '_register_state_dict_hook',\n",
       " '_replicate_for_data_parallel',\n",
       " '_save_to_state_dict',\n",
       " '_slow_forward',\n",
       " '_state_dict_hooks',\n",
       " '_state_dict_pre_hooks',\n",
       " '_version',\n",
       " '_wrapped_call_impl',\n",
       " 'accumulated_bias',\n",
       " 'add_caching_hooks',\n",
       " 'add_hook',\n",
       " 'add_module',\n",
       " 'add_perma_hook',\n",
       " 'all_composition_scores',\n",
       " 'all_head_labels',\n",
       " 'apply',\n",
       " 'b_K',\n",
       " 'b_O',\n",
       " 'b_Q',\n",
       " 'b_U',\n",
       " 'b_V',\n",
       " 'b_in',\n",
       " 'b_out',\n",
       " 'bfloat16',\n",
       " 'blocks',\n",
       " 'buffers',\n",
       " 'cache_all',\n",
       " 'cache_some',\n",
       " 'call_super_init',\n",
       " 'center_unembed',\n",
       " 'center_writing_weights',\n",
       " 'cfg',\n",
       " 'check_and_add_hook',\n",
       " 'check_hooks_to_add',\n",
       " 'children',\n",
       " 'clear_contexts',\n",
       " 'compile',\n",
       " 'context_level',\n",
       " 'cpu',\n",
       " 'cuda',\n",
       " 'dataset',\n",
       " 'double',\n",
       " 'dump_patches',\n",
       " 'embed',\n",
       " 'eval',\n",
       " 'extra_repr',\n",
       " 'fill_missing_keys',\n",
       " 'float',\n",
       " 'fold_layer_norm',\n",
       " 'fold_value_biases',\n",
       " 'forward',\n",
       " 'from_pretrained',\n",
       " 'from_pretrained_no_processing',\n",
       " 'generate',\n",
       " 'get_buffer',\n",
       " 'get_caching_hooks',\n",
       " 'get_extra_state',\n",
       " 'get_parameter',\n",
       " 'get_submodule',\n",
       " 'get_token_position',\n",
       " 'half',\n",
       " 'hook_dict',\n",
       " 'hook_embed',\n",
       " 'hook_points',\n",
       " 'hook_pos_embed',\n",
       " 'hooks',\n",
       " 'init_weights',\n",
       " 'input_to_embed',\n",
       " 'ipu',\n",
       " 'is_caching',\n",
       " 'ln_final',\n",
       " 'load_and_process_state_dict',\n",
       " 'load_sample_training_dataset',\n",
       " 'load_state_dict',\n",
       " 'loss_fn',\n",
       " 'mod_dict',\n",
       " 'modules',\n",
       " 'move_model_modules_to_device',\n",
       " 'mps',\n",
       " 'named_buffers',\n",
       " 'named_children',\n",
       " 'named_modules',\n",
       " 'named_parameters',\n",
       " 'parameters',\n",
       " 'pos_embed',\n",
       " 'process_weights_',\n",
       " 'refactor_factored_attn_matrices',\n",
       " 'register_backward_hook',\n",
       " 'register_buffer',\n",
       " 'register_forward_hook',\n",
       " 'register_forward_pre_hook',\n",
       " 'register_full_backward_hook',\n",
       " 'register_full_backward_pre_hook',\n",
       " 'register_load_state_dict_post_hook',\n",
       " 'register_module',\n",
       " 'register_parameter',\n",
       " 'register_state_dict_pre_hook',\n",
       " 'remove_all_hook_fns',\n",
       " 'requires_grad_',\n",
       " 'reset_hooks',\n",
       " 'run_with_cache',\n",
       " 'run_with_hooks',\n",
       " 'sample_datapoint',\n",
       " 'set_extra_state',\n",
       " 'set_tokenizer',\n",
       " 'set_use_attn_in',\n",
       " 'set_use_attn_result',\n",
       " 'set_use_hook_mlp_in',\n",
       " 'set_use_split_qkv_input',\n",
       " 'setup',\n",
       " 'share_memory',\n",
       " 'state_dict',\n",
       " 'to',\n",
       " 'to_empty',\n",
       " 'to_single_str_token',\n",
       " 'to_single_token',\n",
       " 'to_str_tokens',\n",
       " 'to_string',\n",
       " 'to_tokens',\n",
       " 'tokenizer',\n",
       " 'tokens_to_residual_directions',\n",
       " 'train',\n",
       " 'training',\n",
       " 'type',\n",
       " 'unembed',\n",
       " 'xpu',\n",
       " 'zero_grad']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0-11): 12 x TransformerBlock(\n",
       "    (ln1): LayerNormPre(\n",
       "      (hook_scale): HookPoint()\n",
       "      (hook_normalized): HookPoint()\n",
       "    )\n",
       "    (ln2): LayerNormPre(\n",
       "      (hook_scale): HookPoint()\n",
       "      (hook_normalized): HookPoint()\n",
       "    )\n",
       "    (attn): Attention(\n",
       "      (hook_k): HookPoint()\n",
       "      (hook_q): HookPoint()\n",
       "      (hook_v): HookPoint()\n",
       "      (hook_z): HookPoint()\n",
       "      (hook_attn_scores): HookPoint()\n",
       "      (hook_pattern): HookPoint()\n",
       "      (hook_result): HookPoint()\n",
       "    )\n",
       "    (mlp): MLP(\n",
       "      (hook_pre): HookPoint()\n",
       "      (hook_post): HookPoint()\n",
       "    )\n",
       "    (hook_attn_in): HookPoint()\n",
       "    (hook_q_input): HookPoint()\n",
       "    (hook_k_input): HookPoint()\n",
       "    (hook_v_input): HookPoint()\n",
       "    (hook_mlp_in): HookPoint()\n",
       "    (hook_attn_out): HookPoint()\n",
       "    (hook_mlp_out): HookPoint()\n",
       "    (hook_resid_pre): HookPoint()\n",
       "    (hook_resid_mid): HookPoint()\n",
       "    (hook_resid_post): HookPoint()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([768, 3072])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.blocks[8].mlp.W_in.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
